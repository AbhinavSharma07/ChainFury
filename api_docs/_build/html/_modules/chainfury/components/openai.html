<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>chainfury.components.openai &mdash; ChainFury 0.1.1 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            ChainFury
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installing ChainFury</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.cli.html">chainfury CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.components.html">chainfury Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.agent.html">Agent File</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.base.html">chainfury.base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.client.html">chainfury.client</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.utils.html">chainfury.utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source/chainfury.version.html">chainfury.version module</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">ChainFury</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">chainfury.components.openai</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for chainfury.components.openai</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Dict</span>

<span class="kn">from</span> <span class="nn">chainfury</span> <span class="kn">import</span> <span class="n">Secret</span><span class="p">,</span> <span class="n">model_registry</span><span class="p">,</span> <span class="n">exponential_backoff</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">UnAuthException</span>


<div class="viewcode-block" id="openai_completion"><a class="viewcode-back" href="../../../source/chainfury.components.openai.html#chainfury.components.openai.openai_completion">[docs]</a><span class="k">def</span> <span class="nf">openai_completion</span><span class="p">(</span>
    <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Secret</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]],</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">echo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">stop</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">best_of</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">user</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">raw</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate text completion using OpenAI&#39;s GPT-3 API.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (str): ID of the model to use.</span>
<span class="sd">        prompt (Union[str, List[Union[str, List[str]]]]): The prompt(s) to generate completions for.</span>
<span class="sd">            Encoded as a string, array of strings, array of tokens, or array of token arrays.</span>
<span class="sd">        max_tokens (Optional[int]): The maximum number of tokens to generate in the completion. Defaults to 16.</span>
<span class="sd">        temperature (Optional[float]): What sampling temperature to use, between 0 and 2. Defaults to 1.</span>
<span class="sd">        top_p (Optional[float]): An alternative to sampling with temperature. Defaults to 1.</span>
<span class="sd">        n (Optional[int]): How many completions to generate for each prompt. Defaults to 1.</span>
<span class="sd">        logprobs (Optional[int]): Include the log probabilities on the logprobs most likely tokens.</span>
<span class="sd">            The maximum value for logprobs is 5. Defaults to None.</span>
<span class="sd">        echo (Optional[bool]): Echo back the prompt in addition to the completion. Defaults to False.</span>
<span class="sd">        stop (Optional[Union[str, List[str]]]): Up to 4 sequences where the API will stop generating further tokens.</span>
<span class="sd">            The returned text will not contain the stop sequence. Defaults to None.</span>
<span class="sd">        presence_penalty (Optional[float]): Number between -2.0 and 2.0. Positive values penalize new tokens based on</span>
<span class="sd">            whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.</span>
<span class="sd">            Defaults to 0.</span>
<span class="sd">        frequency_penalty (Optional[float]): Number between -2.0 and 2.0. Positive values penalize new tokens based on</span>
<span class="sd">            their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line</span>
<span class="sd">            verbatim. Defaults to 0.</span>
<span class="sd">        best_of (Optional[int]): Generates best_of completions server-side and returns the &quot;best&quot;.</span>
<span class="sd">            Results cannot be streamed. Defaults to 1.</span>
<span class="sd">        logit_bias (Optional[dict]): Modify the likelihood of specified tokens appearing in the completion.</span>
<span class="sd">            Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated</span>
<span class="sd">            bias value from -100 to 100. Defaults to None.</span>
<span class="sd">        user (Optional[str]): A unique identifier representing your end-user, which can help OpenAI to monitor and detect</span>
<span class="sd">            abuse. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Any: The completion(s) generated by the API.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_fn</span><span class="p">():</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="s2">&quot;https://api.openai.com/v1/completions&quot;</span><span class="p">,</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="n">openai_api_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span><span class="p">,</span>
                <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
                <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
                <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span>
                <span class="s2">&quot;logprobs&quot;</span><span class="p">:</span> <span class="n">logprobs</span><span class="p">,</span>
                <span class="s2">&quot;echo&quot;</span><span class="p">:</span> <span class="n">echo</span><span class="p">,</span>
                <span class="s2">&quot;stop&quot;</span><span class="p">:</span> <span class="n">stop</span><span class="p">,</span>
                <span class="s2">&quot;presence_penalty&quot;</span><span class="p">:</span> <span class="n">presence_penalty</span><span class="p">,</span>
                <span class="s2">&quot;frequency_penalty&quot;</span><span class="p">:</span> <span class="n">frequency_penalty</span><span class="p">,</span>
                <span class="s2">&quot;best_of&quot;</span><span class="p">:</span> <span class="n">best_of</span><span class="p">,</span>
                <span class="s2">&quot;logit_bias&quot;</span><span class="p">:</span> <span class="n">logit_bias</span><span class="p">,</span>
                <span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">user</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">401</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">UnAuthException</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI API returned status code </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">exponential_backoff</span><span class="p">(</span><span class="n">_fn</span><span class="p">)</span></div>


<span class="n">model_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span>
        <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;openai&quot;</span><span class="p">,</span>
        <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;openai-completion&quot;</span><span class="p">,</span>
        <span class="n">fn</span><span class="o">=</span><span class="n">openai_completion</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.&quot;</span><span class="p">,</span>
        <span class="n">usage</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;usage&quot;</span><span class="p">,</span> <span class="s2">&quot;total_tokens&quot;</span><span class="p">],</span>
    <span class="p">),</span>
<span class="p">)</span>


<div class="viewcode-block" id="openai_chat"><a class="viewcode-back" href="../../../source/chainfury.components.openai.html#chainfury.components.openai.openai_chat">[docs]</a><span class="k">def</span> <span class="nf">openai_chat</span><span class="p">(</span>
    <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Secret</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">stop</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">user</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">retry_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">max_retry_delay</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a JSON object containing the OpenAI&#39;s API chat response.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.</span>
<span class="sd">        messages: A list of messages describing the conversation so far, each item contains the folowing keys</span>
<span class="sd">            role: The role of the author of this message. One of system, user, or assistant.</span>
<span class="sd">            content: The contents of the message.</span>
<span class="sd">            name: Optional. The name of the author of this message. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.</span>
<span class="sd">        temperature: Optional. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. Defaults to 1.</span>
<span class="sd">        top_p: Optional. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. Defaults to 1.</span>
<span class="sd">        n: Optional. How many chat completion choices to generate for each input message. Defaults to 1.</span>
<span class="sd">        stop: Optional. Up to 4 sequences where the API will stop generating further tokens.</span>
<span class="sd">        max_tokens: Optional. The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model&#39;s context length. Defaults to infinity.</span>
<span class="sd">        presence_penalty: Optional. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics. See more information about frequency and presence penalties. Defaults to 0.</span>
<span class="sd">        frequency_penalty: Optional. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim. See more information about frequency and presence penalties. Defaults to 0.</span>
<span class="sd">        logit_bias: Optional. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant</span>
<span class="sd">        user: Optional. A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Any: The completion(s) generated by the API.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_fn</span><span class="p">():</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="s2">&quot;https://api.openai.com/v1/chat/completions&quot;</span><span class="p">,</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span>
                <span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="n">openai_api_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="n">json</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
                <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="n">max_tokens</span><span class="p">,</span>
                <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
                <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="n">top_p</span><span class="p">,</span>
                <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="n">n</span><span class="p">,</span>
                <span class="s2">&quot;stop&quot;</span><span class="p">:</span> <span class="n">stop</span><span class="p">,</span>
                <span class="s2">&quot;presence_penalty&quot;</span><span class="p">:</span> <span class="n">presence_penalty</span><span class="p">,</span>
                <span class="s2">&quot;frequency_penalty&quot;</span><span class="p">:</span> <span class="n">frequency_penalty</span><span class="p">,</span>
                <span class="s2">&quot;logit_bias&quot;</span><span class="p">:</span> <span class="n">logit_bias</span><span class="p">,</span>
                <span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">user</span><span class="p">,</span>
            <span class="p">},</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">401</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">UnAuthException</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI API returned status code </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">exponential_backoff</span><span class="p">(</span><span class="n">_fn</span><span class="p">)</span></div>


<span class="n">model_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">Model</span><span class="p">(</span>
        <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;openai&quot;</span><span class="p">,</span>
        <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;openai-chat&quot;</span><span class="p">,</span>
        <span class="n">fn</span><span class="o">=</span><span class="n">openai_chat</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Given a list of messages describing a conversation, the model will return a response.&quot;</span><span class="p">,</span>
        <span class="n">usage</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;usage&quot;</span><span class="p">,</span> <span class="s2">&quot;total_tokens&quot;</span><span class="p">],</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, NimbleBox Engineering.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>